{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "gRekjNpFDlwo",
        "nH1AXjofE1G_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECSE 415 Final Project: Aman Sidhu (260885556) Partick Ohl (261182154)"
      ],
      "metadata": {
        "id": "InxpsU9DCw4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "gRekjNpFDlwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKqu4Tli8YZQ",
        "outputId": "ad47e43f-0624-486f-b3aa-e7a7ea11b368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.1.47-py3-none-any.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.4/750.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238 ultralytics-8.1.47\n"
          ]
        }
      ],
      "source": [
        "# Install ultralytics for YOLOv8n\n",
        "%pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "from ultralytics.solutions import object_counter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_folder = '/content/drive/My Drive/Colab Notebooks/ECSE415/Assignment6/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_tjTc5fFsLM",
        "outputId": "56dc9459-37db-47c7-d160-6ea2a4a8f3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions for Report\n",
        "def print_img(img, name = \"\", cmap = plt.get_cmap('gray')):\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    print(name + \" Shape:\", np.shape(img))\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, title):\n",
        "    class_names = ['Negative', 'Positive']\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(conf_matrix, interpolation='nearest', cmap='viridis')\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(conf_matrix.shape[1]),\n",
        "           yticks=np.arange(conf_matrix.shape[0]),\n",
        "           xticklabels=class_names[::-1], yticklabels=class_names[::-1],  # Reversed order\n",
        "           title=\"Confusion Matrix for \" + title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(conf_matrix.shape[0]):\n",
        "        for j in range(conf_matrix.shape[1]):\n",
        "            ax.text(j, i, format(conf_matrix[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2 else \"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def calculate_metrics(TP, FP, TN, FN):\n",
        "    recall = TP / (TP + FN)\n",
        "    precision = TP / (TP + FP)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return recall, precision, f1_score"
      ],
      "metadata": {
        "id": "JlFqG74qEGAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of Overall Approach and Assumptions"
      ],
      "metadata": {
        "id": "JAPvHXGREdzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach consist of 2 separate tasks:\n",
        "\n",
        "\n",
        "**Identifying/Counting passed cars and people**\n",
        "\n",
        "We keep track of the id's of any cars and people that we passed.\n",
        "\n",
        "We assume that we pass an object according to the following rules:\n",
        "Any person whose bounding box center is anywhere in the bottom 40% of the screen will be considered as passed, and for cars it is 26.5%. From our testing, this helped with situations where objects might only be detected in the few frames before they are passed, and reduce instances where occlusion in the environment caused by moving cars and our moving perspective removes the chance of detecting an object. Tighter thresholds would perhaps be more rigorous but greatly reduce the window to detect objects. This threshold also helps reduce the times we recount the same object twice, and count objects near the end of the video that we would not have passed yet. We also assume that any person detected is a pedestrian, which would also include any bikers.\n",
        "\n",
        "**Classifying passed cars as moving/parked**\n",
        "\n",
        "The classification consists of 2 stages:\n",
        "\n",
        "\n",
        "\n",
        "1.   **Initial Guess:**\n",
        "  Based on the observation that moving cars are generally driving in the middle of the road (following a certain path), while parked cars are on the side of the road, we make the assumption that every car (we pass) that passes through a certain region of the screen is a moving car. We thus classify every car that passes through a certain line as moving and every other car as parkerd. This gives us a usable first guess for the classification.\n",
        "\n",
        "2.   **Refinement Stage:**\n",
        "This stage is based on the flow of the cars. We observed that the flow magnitudes have the general ordering\n",
        "```\n",
        "flow(moving_same_dir) < flow(parked) < flow(moving_opposite_dir)\n",
        "```\n",
        "\n",
        "which confirms the idea that the speed of the cars relative to the dashcam (which follow above ordering) are reflected in the flow.\n",
        "\n",
        "We use this observation along with the assumption that all moving cars that we pass move in the same direction as each other, which is a fair assumption when driving in the city (due to the restricted size of the streets).\n",
        "So we use K-means clustering with K=2 and our previous guess as initial labels in order to identify cars that the previous method may have misclassified. The resulting labels are our final classification.\n",
        "\n",
        "**Note:** We define the flow magnitude of a car as the maximum of all mean flow magnitudes (excluding 0 flows) in the bounding boxes (for every frame) of a car. This is the flow-based feature we found that reflects above ordering the most consistently. When including other features such as flow-angle or the standard deviation in flow-angle, the results got worse.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DnXkSL-fEfjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Software Packages Used"
      ],
      "metadata": {
        "id": "X8F6546vEow4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLO (Ultralytics)**\n",
        "You Only Look Once (YOLO), is a popular object detection and image segmentation model, originating from a paper called \"You Only Look Once: Unified, Real-Time Object Detection\" by Joseph Redmon et al. from the University of Washington. The original YOLO implementation uses a single CNN to predict bounding boxes and class probabilities directly from images in one evaluation. It first divides the image into a SxS grid, where it tries to find a suitable bounding box in each cell and the corresponding class confidence. Ultralytics provides an improved version of YOLO capable of providing fast, meaningful results and has a variety of extra features. Some features include detection, segmentation, pose estimation, tracking, and classification. For our purposes, we use a YOLOv8 model, which is pre-trained on the COCO dataset, for object classification, tracking, bounding box generation, and object counting.\n",
        "\n",
        "**OpenCV**\n",
        "Open Source Computer Vision Library (OpenCV) is a very popular computer vision library that provides many essential functionalities such as image processing, and feature extraction. In this project, OpenCV is used for\n",
        "* reading the frames of the video\n",
        "* preprocessing the frames\n",
        "* K-means Clustering"
      ],
      "metadata": {
        "id": "ex33N7_NEqsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program Results"
      ],
      "metadata": {
        "id": "nH1AXjofE1G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provides initial analytics of the video\n",
        "class Analyzer:\n",
        "  def __init__(self, video_path):\n",
        "      self.cap = cv2.VideoCapture(video_path)\n",
        "      w, h = (int(self.cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "      self.frame_shape = (h, w)\n",
        "\n",
        "      self.tracking_model = YOLO('yolov8n.pt')\n",
        "      self.names = self.tracking_model.names\n",
        "\n",
        "      self.passed_cars = set()\n",
        "      self.tracked_flows = {}\n",
        "\n",
        "      self.passed_people = set()\n",
        "\n",
        "      self.bottom_threshold_cars = int(0.735*h)\n",
        "      self.bottom_threshold_person = int(0.6*h)\n",
        "      self.car_confidence_threshold = 0.8\n",
        "      self.person_confidence_threshold = 0.6\n",
        "\n",
        "      self.classes_to_count = [0, 2, 5, 7] # 0: person, 2: car, 5: bus, 7: truck\n",
        "      self.count_names = {i: self.tracking_model.names[i] for i in self.classes_to_count}\n",
        "\n",
        "      self.counter_moving_cars = object_counter.ObjectCounter()\n",
        "      line_points = [(int(w*0.2), int(0.762*h)), (int(w*0.8), int(0.9125*h))]\n",
        "      self.counter_moving_cars.set_args(view_img = True, reg_pts = line_points, classes_names = self.count_names, draw_tracks=True)\n",
        "\n",
        "\n",
        "  def compute_flow(self, img1, img2, flow):\n",
        "    return cv2.calcOpticalFlowFarneback(prev=img1,\n",
        "                                        next=img2,\n",
        "                                        flow=flow,\n",
        "                                        pyr_scale=0.5,\n",
        "                                        levels=3,\n",
        "                                        winsize=15,\n",
        "                                        iterations=3,\n",
        "                                        poly_n=5,\n",
        "                                        poly_sigma=1.2,\n",
        "                                        flags=0 if flow is None else cv2.OPTFLOW_USE_INITIAL_FLOW\n",
        "                                        )\n",
        "\n",
        "\n",
        "  # Returns True if car, optionally adds it to passed_cars\n",
        "  def check_passed_car(self, id, cls, box, conf):\n",
        "    conf = math.ceil(conf*100)/100\n",
        "\n",
        "    is_car = self.names[int(cls)] in ('bus', 'car', 'truck') and conf > self.car_confidence_threshold\n",
        "    if is_car and id:\n",
        "      x_tl, y_tl, x_br, y_bl = box\n",
        "      if (y_tl + abs(y_tl - y_bl)//2) > self.bottom_threshold_cars:\n",
        "        self.passed_cars.add(id)\n",
        "    return is_car\n",
        "\n",
        "\n",
        "  # Adds person to passed_people if we (are going to) pass them\n",
        "  def check_passed_person(self, id, cls, box, conf):\n",
        "    if self.names[int(cls)] == 'person' and conf > self.person_confidence_threshold:\n",
        "            x_tl, y_tl, x_br, y_bl = box\n",
        "            if (y_tl + abs(y_tl - y_bl)//2) > self.bottom_threshold_person:\n",
        "              self.passed_people.add(id)\n",
        "\n",
        "\n",
        "  def analyse_frame_data(self, tracks, flow_mags, flow_angles):\n",
        "      flow_not_zero_mask = np.zeros(self.frame_shape)\n",
        "      flow_not_zero_mask[flow_mags > 0] = 1\n",
        "\n",
        "      boxes = tracks[0].boxes.xyxy.cpu()\n",
        "      clss = tracks[0].boxes.cls.cpu().tolist()\n",
        "      confs = tracks[0].boxes.conf.cpu().tolist()\n",
        "      trk_ids = tracks[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "      for (box, clss, trk_id, conf) in zip(boxes, clss, trk_ids, confs):\n",
        "          trk_id = int(trk_id)\n",
        "          # Count passed cars\n",
        "          if self.check_passed_car(trk_id, clss, box, conf):\n",
        "            # Compute flow data for every tracked car\n",
        "            # Get tracking box as mask\n",
        "            box_mask = np.zeros(self.frame_shape)\n",
        "            box_mask[int(box[1]):int(box[3]), int(box[0]):int(box[2])] = 1\n",
        "            # Get flow for tracked object\n",
        "            n_flows_in_box = np.sum(box_mask*flow_not_zero_mask)\n",
        "            mean_flow_mag = np.sum(flow_mags * box_mask) / n_flows_in_box\n",
        "            # Update tracked flow features\n",
        "            if trk_id in self.tracked_flows:\n",
        "                self.tracked_flows[trk_id] = max(self.tracked_flows[trk_id], mean_flow_mag)\n",
        "            else:\n",
        "                self.tracked_flows[trk_id] = mean_flow_mag\n",
        "          else:\n",
        "            # Count passed persons\n",
        "            self.check_passed_person(trk_id, clss, box, conf)\n",
        "\n",
        "\n",
        "  def preprocess_frame(self, frame):\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      frame = cv2.equalizeHist(frame) # To improve robustness against lighting changes\n",
        "      return frame\n",
        "\n",
        "\n",
        "  def analyse(self):\n",
        "    # Get first frame\n",
        "    _, prev_frame = self.cap.read()\n",
        "    prev_processed_frame = self.preprocess_frame(prev_frame)\n",
        "\n",
        "    flow = None\n",
        "    while True:\n",
        "        # Get the next frame\n",
        "        ret, frame = self.cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Preprocessing\n",
        "        processed_frame = self.preprocess_frame(frame)\n",
        "        # Compute dense optical flow\n",
        "        flow = self.compute_flow(prev_processed_frame, processed_frame, flow)\n",
        "        magnitude, angle = cv2.cartToPolar(flow[:, :, 0], flow[:, :, 1])\n",
        "        # Track objects\n",
        "        results = self.tracking_model.track(frame, classes=self.classes_to_count, persist=True, verbose=False, show=False)\n",
        "        # Get first assumption of driving/parked\n",
        "        self.counter_moving_cars.start_counting(frame, results)\n",
        "        # Get corresponding flows\n",
        "        self.analyse_frame_data(results, magnitude, angle)\n",
        "        # Update previous frame\n",
        "        prev_processed_frame = processed_frame\n",
        "\n",
        "    self.cap.release()\n",
        "\n",
        "  def get_results(self):\n",
        "    return (self.passed_cars, self.counter_moving_cars.count_ids, self.tracked_flows, self.passed_people)"
      ],
      "metadata": {
        "id": "iZ2RfIgM84hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_results(video_name, analyzer):\n",
        "  passed_cars, moving_ids, tracked_flows, passed_people = analyzer.get_results()\n",
        "\n",
        "  car_ids = np.zeros(len(passed_cars))\n",
        "  car_features = np.zeros((len(passed_cars)))\n",
        "\n",
        "  # Convert flow data to numpy array corresponding to car ids\n",
        "  for (i, c) in enumerate(passed_cars):\n",
        "    max_mean_flow_mag = tracked_flows[c]\n",
        "\n",
        "    car_ids[i] = c\n",
        "    car_features[i] = max_mean_flow_mag\n",
        "  # Normalize\n",
        "  norm = np.linalg.norm(car_features, axis=0, keepdims=True)\n",
        "  norm_car_features = car_features / norm\n",
        "\n",
        "  # Get initial car labels (parked/moving)\n",
        "  labels = np.zeros(norm_car_features.shape[0], dtype=np.int32)\n",
        "  # For all cars that were detected as moving, change the label\n",
        "  for id in moving_ids:\n",
        "    # The id, features and label of a car are located at the same index\n",
        "    car_index = np.where(car_ids == id)[0]\n",
        "    labels[car_index] = 1\n",
        "\n",
        "  # Apply clustering based on flow in order to refine results\n",
        "  criteria = (cv2.TERM_CRITERIA_MAX_ITER+cv2.TERM_CRITERIA_EPS, 20, 0.05)\n",
        "  cv2.kmeans(norm_car_features.astype(np.float32), K=2, bestLabels=labels,\n",
        "                                            criteria=criteria, attempts=1, flags=cv2.KMEANS_USE_INITIAL_LABELS)\n",
        "  parked_car_ids = car_ids[labels == 0]\n",
        "  moving_car_ids = car_ids[labels == 1]\n",
        "\n",
        "  # Output\n",
        "  print('Results: ', video_name)\n",
        "  print('# Passed Parked Cars: ', parked_car_ids.shape[0])\n",
        "  print('# Passed Moving Cars: ', moving_car_ids.shape[0])\n",
        "  print('# Passed People     : ', len(passed_people))"
      ],
      "metadata": {
        "id": "ZZ3Z23zMh8G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer_mcgill = Analyzer(base_folder + 'mcgill_drive.mp4')\n",
        "analyzer_mcgill.analyse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz8WrJP7RLtz",
        "outputId": "96a94447-cc43-4a8b-ff10-2c35c3fc5b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "Line Counter Initiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer_st_cath = Analyzer(base_folder + 'st-catherines_drive.mp4')\n",
        "analyzer_st_cath.analyse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yg6jKxRBFRFk",
        "outputId": "8a3d6488-7baf-4f73-f1c9-5ea373a7ef03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "Line Counter Initiated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_results('mcgill_drive.mp4', analyzer_mcgill)\n",
        "print()\n",
        "evaluate_results('st-catherines_drive.mp4', analyzer_st_cath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCpmCHJxFMxV",
        "outputId": "f40b47ff-b4c0-4a00-be13-fc12df5bfecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results:  mcgill_drive.mp4\n",
            "# Passed Parked Cars:  10\n",
            "# Passed Moving Cars:  23\n",
            "# Passed People     :  21\n",
            "\n",
            "Results:  st-catherines_drive.mp4\n",
            "# Passed Parked Cars:  50\n",
            "# Passed Moving Cars:  4\n",
            "# Passed People     :  61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "St. Catherine\n",
        "* Total Number of Passed Moving Cars: 4\n",
        "* Total Number of Passed Parked Cars: 50\n",
        "* Total Number of Pedestrians: 61\n",
        "\n",
        "McGill Drive\n",
        "* Total Number of Passed Moving Cars: 23\n",
        "* Total Number of Passed Parked Cars: 10\n",
        "* Total Number of Pedestrians: 35\n",
        "\n",
        "**Ground Truth:**\n",
        "\n",
        "St. Catherine\n",
        "* Total Number of Passed Moving Cars: 2\n",
        "* Total Number of Passed Parked Cars: 55\n",
        "* Total Number of Pedestrians: 104\n",
        "\n",
        "McGill Drive\n",
        "* Total Number of Passed Moving Cars: 24\n",
        "* Total Number of Passed Parked Cars: 10\n",
        "* Total Number of Pedestrians: 21\n",
        "\n",
        "Since these results were obtained manually, here is our process for classifying each. Pedestrians are any person either on the sidewalk, beside a car on the road, or crossing the street that is visible from the dashcam. Since bikers could be incorrectly detected as a person thus a false positive, when they are not detected they are true negative examples of pedestrians. While the definition for moving and parked cars are more straightforward, given by their names, stopped cars such as those at intersections waiting for a green light are not. Since they technically are not moving but also still in use, we exclude them from our analysis due to this discrepancy. True negatives for moving cars is the number of cars correctly classified as not moving (parked), and likewise for parked cars it is the amount cars marked as not parked (moving)."
      ],
      "metadata": {
        "id": "msabMBwdFCyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion of Program Performance and Problems"
      ],
      "metadata": {
        "id": "zkX3jFOJG8SV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results, using a YOLO back-bone for the object detection seems to provide overall decent results with car, truck, and bus detection. Upon reviewing the annotated videos for each video, many distinct and clear examples of cars are correctly detected in both cases which is reflected in the number of total cars detected. For pedestrians, performance was generally good even in the more complicated St. Catherines video, however the model was not able to detect people consistently in poor lighting conditions. In general, occlusion did prevent some instances of people not being detected. In this case, perhaps a change to a more robust back-bone like Mask R-CNN could have provide better person detection. False positive of pedestrians were also kept low, but one or two bikers, random street objects, and counting an object twice did still occur.\n",
        "\n",
        "Object tracking generally performed very well in both scenes with only a few cases where objects were sometimes classified with a new ID tag despite already being seen before. This was mitigated with decision to only count cars as they pass through the bottom of the screen. However, the low threshold for cars tended to exclude large vehicles like buses and trucks whose centers tended to be higher as they moved out of view.\n",
        "\n",
        "The object_counter object used to get the initial classification of moving/parked cars is able to capture many cars that pass us, but can still be improved. Since moving cars and parked cars can sometime occupy the same region of the screen, we opted to be more conservative with the size of the line to avoid false positive moving cars. This meant that certain vehicles bounding box centers could miss the line and thus be misclassified as a parked car instead. Additionally, stopped cars moving the perpendicular direction at intersections may be incorrectly classified as parked since they appear on similar areas of the screen.\n",
        "However, the refinement process could correct most of these mistakes, which became especially clear in the mcgill_drive video, where the initial guess was very inaccurate. The dense flow estimation itself is very computationally intensive, which made testing more difficult. However, since the flow does not uniquely depend on the speed of the cars relative to the dashcam, but may also depend on the specific features observable in the bounding box and the angle at which a car is captured, the method may still be improved."
      ],
      "metadata": {
        "id": "UnBBOShRHAeU"
      }
    }
  ]
}