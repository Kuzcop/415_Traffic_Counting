{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECSE 415 Final Project: Aman Sidhu (260885556) Partick Ohl ()"
      ],
      "metadata": {
        "id": "YjhKI2RScgSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JsA_6p69cwZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQRH23gl_Xhq",
        "outputId": "9a935db4-8b84-4c87-9a0d-09d512535050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/Olympus in the Sky/McGill/2024 - Winter/ECSE 415/Final Project/'\n",
        "\n",
        "video_1 = 'st-catherines_drive.mp4'\n",
        "video_2 = 'mcgill_drive.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg\n",
        "import numpy             as np\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def print_img(img, name = \"\", cmap = plt.get_cmap('gray')):\n",
        "    plt.imshow(img, cmap=cmap)\n",
        "    print(name + \" Shape:\", np.shape(img))\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "!pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.solutions import object_counter\n",
        "import cv2, math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r8QHclzdL4H",
        "outputId": "bc90aaba-82b0-41be-b35d-59f9ee9ba4d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.1.47-py3-none-any.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.4/750.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238 ultralytics-8.1.47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#detailed description of the overall approach taken. State clearly any assumptions that you made.\n",
        "#descriptions of each software package or routine used\n",
        "#summary of program output on the two videos, with comparison to manually obtained ground truth values\n",
        "#discussion of program performance and problems"
      ],
      "metadata": {
        "id": "-0R2c2JVeDYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Description of Overall Approach and Assumptions"
      ],
      "metadata": {
        "id": "AButyease22C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approach makes extensive use of the YOLO model features and functions from Ultralytics to perform key computer vision techniques needed for this problem: object detection/classification, multi-object tracking, and bounding box generation. Specifically, we use the pre-trained YOLOv8 model which is capable of these tasks, while providing fast and accurate results. Futhermore, as we are trying to detect number of passed cars and people in the scene, we limit the object detection to only focus on 'car', 'bus', 'truck', and 'person' objects.\n",
        "\n",
        "Counting unique objects first involves creating a python set variable, an array which excludes repetitions, for both cars and pedestrians, respectively. In both sets, we include the tracking id of each passed pedestrian, and car even if it is parked or moving. This arises our first assumption: any person whose bounding box center is anywhere in the bottom 40% of the screen will be considered as passed, and for cars it is 26.5%. From our testing, this helped with situations where objects might only be detected in the few frames before they are passed, and reduce instances where occlusion in the environment caused by moving cars and our moving perspective removes the chance of detecting an object. Tighter thresholds would perhaps be more rigorous but greatly reduce the window to detect objects. This threshold also helps reduce the times we recount the same object twice, and count objects near the end of the video that we would not have passed yet. We also assume that any person detected is a pedestrian, which would also include any bikers. Lastly, to distinguish between moving and parked cars we use object_counter object from Ultralytics to count the tracking IDs of cars whose bounding box centers pass through a certain region of the screen, specifically a line. This approach tries to exploit how moving cars that we pass generally follow a path across the screen. This line is angled and positioned near the very bottom of the screen to ensure that only passed, moving cars are counted. This separate set of tracking IDs is then removed from the set of all car IDs to provide the final number of parked and moving cars."
      ],
      "metadata": {
        "id": "mc6k9QdEj6Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Software Packages Used"
      ],
      "metadata": {
        "id": "qp0znII9etva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLO (Ultralytics)**\n",
        "You Only Look Once (YOLO), is a popular object detection and image segmentation model, originating from a paper called \"You Only Look Once: Unified, Real-Time Object Detection\" by Joseph Redmon et al. from the University of Washington. The original YOLO implementation uses a single CNN to predict bounding boxes and class probabilities directly from images in one evaluation. It first divides the image into a SxS grid, where it tries to find a suitable bounding box in each cell and the corresponding class confidence. Ultralytics provides an improved version of YOLO capable of providing fast, meaningful results and has a variety of extra features. Some features include detection, segmentation, pose estimation, tracking, and classification. For our purposes, we are using the object classification, tracking, bounding box generation, and object_counter functionality from Ultralytics.\n",
        "\n",
        "**OpenCV**\n",
        "Open Source Computer Vision Library (OpenCV) is a very popular computer vision library that provides many additional functionalities such as image processing, and feature extraction. In this project, OpenCV is used to read individual frames from the video which then passed to the YOLO model to perform tracking, object detection, line counting, and bounding box generation."
      ],
      "metadata": {
        "id": "GxHWEFkIhcIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program Results"
      ],
      "metadata": {
        "id": "vhynhrjMeo6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"yolov8n.pt\")\n",
        "# Yolo, Object_Counter, threshold Arguments\n",
        "classes_to_count            = [0, 2, 5, 7] # 0: person, 2: car, 5: bus, 7: truck\n",
        "car_confidence_threshold    = 0.8\n",
        "person_confidence_threshold = 0.6\n",
        "count_names                 = {i: model.names[i] for i in classes_to_count}\n",
        "\n",
        "for vid_num, video in enumerate((video_1, video_2)):\n",
        "  cap = cv2.VideoCapture(path + video)\n",
        "  assert cap.isOpened(), \"Error reading video file\"\n",
        "  w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "\n",
        "  bottom_threshold_cars       = int(0.735*h)\n",
        "  bottom_threshold_person     = int(0.6*h)\n",
        "\n",
        "  # Init Object Counters\n",
        "  # Tracks the motion of cars that pass through the bottom of the screen\n",
        "  counter_cars = object_counter.ObjectCounter()\n",
        "  line_points = [(int(w*0.2), int(0.762*h)), (int(w*0.8), int(0.9125*h))]\n",
        "  counter_cars.set_args(view_img = True, reg_pts = line_points, classes_names = count_names, draw_tracks=True)\n",
        "\n",
        "  # Video writer\n",
        "  video_writer = cv2.VideoWriter(\"video_{}.avi\".format(vid_num), cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
        "\n",
        "  unique_cars   = set()\n",
        "  unique_people = set()\n",
        "\n",
        "  while cap.isOpened():\n",
        "      success, im0 = cap.read()\n",
        "      if not success:\n",
        "          print(\"Video frame is empty or video processing has been successfully completed.\")\n",
        "          break\n",
        "\n",
        "      # Tracking\n",
        "      tracks = model.track(im0, persist=True, show=False, classes=classes_to_count, verbose = False)\n",
        "\n",
        "      # Process Bounding Boxes\n",
        "      for r in tracks:\n",
        "        for box in r.boxes:\n",
        "          conf = math.ceil(box.conf[0]*100)/100\n",
        "\n",
        "          # Count all cars and people that pass near the bottom 40% of the screen\n",
        "          if r.names[int(box.cls[0])] in ('bus', 'car', 'truck') and conf > car_confidence_threshold:\n",
        "            x_tl, y_tl, x_br, y_bl = box.xyxy[0]\n",
        "            if (y_tl + abs(y_tl - y_bl)//2) > bottom_threshold_cars:\n",
        "              unique_cars.add(int(box.id))\n",
        "\n",
        "          if r.names[int(box.cls[0])] == 'person' and conf > person_confidence_threshold:\n",
        "            x_tl, y_tl, x_br, y_bl = box.xyxy[0]\n",
        "            if (y_tl + abs(y_tl - y_bl)//2) > bottom_threshold_person:\n",
        "              unique_people.add(int(box.id))\n",
        "\n",
        "      im0 = counter_cars.start_counting(im0, tracks)\n",
        "      video_writer.write(im0)\n",
        "\n",
        "  cap.release()\n",
        "  video_writer.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  print('-'*100)\n",
        "  print('Video: ' + video)\n",
        "  all_cars = len(unique_cars)\n",
        "  print('Total Number of Cars: {}'.format(all_cars)) # all cars\n",
        "\n",
        "  for moving_car in counter_cars.count_ids:\n",
        "    unique_cars.discard(moving_car)\n",
        "\n",
        "  parked_cars = len(unique_cars)\n",
        "  print('Total Number of Parked Cars: {}'.format(parked_cars)) # all cars - marked outbound cars\n",
        "  print('Total Number of Moving Cars: {}'.format(len(counter_cars.count_ids))) # number of moving cars\n",
        "  print('Total Number of Pedestrians: {}'.format(len(unique_people))) # number of passed cars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D88gsFjdfwu",
        "outputId": "22cf3d43-e6fb-4c79-c6d3-e4cbc4570c83"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "Line Counter Initiated.\n",
            "Video frame is empty or video processing has been successfully completed.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Video: st-catherines_drive.mp4\n",
            "Total Number of Cars: 54\n",
            "Total Number of Parked Cars: 53\n",
            "Total Number of Moving Cars: 1\n",
            "Total Number of Pedestrians: 61\n",
            "WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n",
            "\n",
            "Line Counter Initiated.\n",
            "Video frame is empty or video processing has been successfully completed.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Video: mcgill_drive.mp4\n",
            "Total Number of Cars: 33\n",
            "Total Number of Parked Cars: 18\n",
            "Total Number of Moving Cars: 15\n",
            "Total Number of Pedestrians: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth:\n",
        "\n",
        "**St. Catherine**\n",
        "* Total Number of Passed Moving Cars: 3\n",
        "* Total Number of Passed Parked Cars: 55\n",
        "* Total Number of Pedestrians: 104\n",
        "\n",
        "**McGill Drive**\n",
        "* Total Number of Passed Moving Cars: 25\n",
        "* Total Number of Passed Parked Cars: 11\n",
        "* Total Number of Pedestrians: 30"
      ],
      "metadata": {
        "id": "u5fTCbhagygD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO: create a confusion matrix to show results, potentially play videos in notebook"
      ],
      "metadata": {
        "id": "T1z4VrXwkCvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion of Program Performance and Problems"
      ],
      "metadata": {
        "id": "25w_7HjFeib7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One flaw in our approach is that do not make the difference between a parked car and stopped cars moving the perpendicular direction at intersections. Since both cars appear on similar areas of the screen and are not moving, these cars are misclassifed in our program as parked. Similarly, bikers are also misclassified as pedestrians. The threshold is not effective with larger vehicles since their bounding box center is generally higher than the threshold as it passes out of view. The object_counter object used to detect moving cars is able to capture many cars that pass us, but since moving cars and parked cars can sometime occupy the same region of the screen,\n",
        "The speed provided by YOLO helped significantly with testing and validating different approaches, however a more accurate model could have provided more consistent results."
      ],
      "metadata": {
        "id": "T8eSjBxwqAoT"
      }
    }
  ]
}